{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSMARCO Document Ranking Task using PyTerrier - uogTrBaseDPH\n",
    "\n",
    "This notebook demonstrates indexing and performing a baseline DPH run for the MSMARCO Document Ranking task using [PyTerrier](https://github.com/terrier-org/pyterrier).\n",
    "\n",
    "Author: Craig Macdonald, University of Glasgow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTerrier Setup\n",
    "\n",
    "We need to install PyTerrier. We can do this using Pip by uncommenting this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade git+https://github.com/terrier-org/pyterrier.git#egg=python-terrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your JAVA_HOME environment variable does not specify a directory for Java 11, you should set it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"JAVA_HOME\"] = \"/local/trmaster/opt/jdk-11.0.6/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now start PyTerrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T09:11:11.397045Z",
     "iopub.status.busy": "2020-08-31T09:11:11.396504Z",
     "iopub.status.idle": "2020-08-31T09:11:13.292614Z",
     "shell.execute_reply": "2020-08-31T09:11:13.291541Z",
     "shell.execute_reply.started": "2020-08-31T09:11:11.396965Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "  pt.init(mem=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "PyTerrier contains a Datasets API that alllows to index/retrieve from a number of standard datasets. We can see which datasets are supported using `pt.list_datasets()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T09:11:18.001152Z",
     "iopub.status.busy": "2020-08-31T09:11:18.000686Z",
     "iopub.status.idle": "2020-08-31T09:11:18.050193Z",
     "shell.execute_reply": "2020-08-31T09:11:18.049228Z",
     "shell.execute_reply.started": "2020-08-31T09:11:18.001081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>topics</th>\n",
       "      <th>qrels</th>\n",
       "      <th>corpus</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50pct</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vaswani</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trec-deep-learning-docs</td>\n",
       "      <td>(train, dev, test, test-2020)</td>\n",
       "      <td>(train, dev, test)</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trec-robust-2004</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trec-robust-2005</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trec-covid</td>\n",
       "      <td>(round1, round2, round3)</td>\n",
       "      <td>(round1, round2)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trec-wt2g</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>trec-wt-2002</td>\n",
       "      <td>(td, np)</td>\n",
       "      <td>(np, td)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trec-wt-2003</td>\n",
       "      <td>(td, np)</td>\n",
       "      <td>(np, td)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trec-wt-2004</td>\n",
       "      <td>(all, np, hp, td)</td>\n",
       "      <td>(hp, td, np, all)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>trec-wt-2009</td>\n",
       "      <td>True</td>\n",
       "      <td>(adhoc, adhoc.catA, adhoc.catB)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>trec-wt-2010</td>\n",
       "      <td>True</td>\n",
       "      <td>(adhoc)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>trec-wt-2011</td>\n",
       "      <td>True</td>\n",
       "      <td>(adhoc)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>trec-wt-2012</td>\n",
       "      <td>True</td>\n",
       "      <td>(adhoc)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    dataset                         topics  \\\n",
       "0                     50pct                           None   \n",
       "1                   vaswani                           True   \n",
       "2   trec-deep-learning-docs  (train, dev, test, test-2020)   \n",
       "3          trec-robust-2004                           True   \n",
       "4          trec-robust-2005                           True   \n",
       "5                trec-covid       (round1, round2, round3)   \n",
       "6                 trec-wt2g                           True   \n",
       "7              trec-wt-2002                       (td, np)   \n",
       "8              trec-wt-2003                       (td, np)   \n",
       "9              trec-wt-2004              (all, np, hp, td)   \n",
       "10             trec-wt-2009                           True   \n",
       "11             trec-wt-2010                           True   \n",
       "12             trec-wt-2011                           True   \n",
       "13             trec-wt-2012                           True   \n",
       "\n",
       "                              qrels corpus index  \n",
       "0                              None   None  True  \n",
       "1                              True   True  True  \n",
       "2                (train, dev, test)   True  None  \n",
       "3                              True   None  None  \n",
       "4                              True   None  None  \n",
       "5                  (round1, round2)   None  None  \n",
       "6                              True   None  None  \n",
       "7                          (np, td)   None  None  \n",
       "8                          (np, td)   None  None  \n",
       "9                 (hp, td, np, all)   None  None  \n",
       "10  (adhoc, adhoc.catA, adhoc.catB)   None  None  \n",
       "11                          (adhoc)   None  None  \n",
       "12                          (adhoc)   None  None  \n",
       "13                          (adhoc)   None  None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MSMARCO document ranking task, the corresponding dataset is `\"trec-deep-learning-docs\"`, which we can see provides various topics and qrels sets, and provides a copy of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T09:13:12.414211Z",
     "iopub.status.busy": "2020-08-31T09:13:12.413734Z",
     "iopub.status.idle": "2020-08-31T09:13:12.419728Z",
     "shell.execute_reply": "2020-08-31T09:13:12.418315Z",
     "shell.execute_reply.started": "2020-08-31T09:13:12.414133Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"trec-deep-learning-docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run `get_corpus()`, it will download the TREC formatted version of the corpus. NB: This is 22GB, so too much for Google Colab unfortunately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T09:15:09.730815Z",
     "iopub.status.busy": "2020-08-31T09:15:09.730340Z",
     "iopub.status.idle": "2020-08-31T09:15:09.818922Z",
     "shell.execute_reply": "2020-08-31T09:15:09.818016Z",
     "shell.execute_reply.started": "2020-08-31T09:15:09.730742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/users/craigm/.pyterrier/corpora/trec-deep-learning-docs/corpus/msmarco-docs.trec.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Lets get setup for indexing. This is a basic configuration, without applying any stopword removal or stremming. Indexing on our machine took just over 1 hour using a single thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T09:44:59.161566Z",
     "iopub.status.busy": "2020-08-31T09:44:59.161103Z",
     "iopub.status.idle": "2020-08-31T10:55:38.905239Z",
     "shell.execute_reply": "2020-08-31T10:55:38.904138Z",
     "shell.execute_reply.started": "2020-08-31T09:44:59.161498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:44:59.728 [main] WARN  o.t.i.MultiDocumentFileCollection - trec.encoding is not set; resorting to platform default (ISO-8859-1). Indexing may be platform dependent. Recommend trec.encoding=UTF-8\n",
      "10:44:59.824 [main] INFO  o.t.i.MultiDocumentFileCollection - TRECCollection 0% processing /users/craigm/.pyterrier/corpora/trec-deep-learning-docs/corpus/msmarco-docs.trec.gz\n",
      "10:44:59.883 [main] INFO  o.t.structures.indexing.Indexer - creating the data structures data_1\n",
      "10:44:59.928 [main] INFO  o.t.s.indexing.LexiconBuilder - LexiconBuilder active - flushing every 100000 documents, or when memory threshold hit\n",
      "11:43:34.096 [main] INFO  o.t.structures.indexing.Indexer - Collection #0 took 3514 seconds to index (3213835 documents)\n",
      "11:44:06.810 [main] INFO  o.t.s.indexing.LexiconBuilder - 33 lexicons to merge\n",
      "11:45:48.357 [main] INFO  o.t.s.indexing.LexiconBuilder - Optimising structure lexicon\n",
      "11:45:48.366 [main] INFO  o.t.s.i.FSOMapFileLexiconUtilities - Optimising lexicon with 17470544 entries\n",
      "11:45:55.715 [main] INFO  o.t.structures.indexing.Indexer - Started building the inverted index...\n",
      "11:45:55.749 [main] INFO  o.t.s.i.c.InvertedIndexBuilder - BasicMemSizeLexiconScanner: lexicon scanning until approx 5.8 GiB of memory is consumed\n",
      "11:45:55.763 [main] INFO  o.t.s.i.c.InvertedIndexBuilder - Iteration 1 of 2 (estimated) iterations\n",
      "11:50:27.839 [main] INFO  o.t.s.i.c.InvertedIndexBuilder - Iteration 2 of 2 (estimated) iterations\n",
      "11:55:22.158 [main] INFO  o.t.s.indexing.LexiconBuilder - Optimising structure lexicon\n",
      "11:55:22.167 [main] INFO  o.t.s.i.FSOMapFileLexiconUtilities - Optimising lexicon with 17470544 entries\n",
      "11:55:38.863 [main] INFO  o.t.structures.indexing.Indexer - Finished building the inverted index...\n",
      "11:55:38.876 [main] INFO  o.t.structures.indexing.Indexer - Time elapsed for inverted file: 583\n"
     ]
    }
   ],
   "source": [
    "!rm -rf index/\n",
    "!mkdir -p index\n",
    "props = {\n",
    "  'indexer.meta.reverse.keys':'docno',\n",
    "  'termpipelines' : '',\n",
    "}\n",
    "\n",
    "pt.logging('INFO')\n",
    "indexer = pt.TRECCollectionIndexer(\"./index\")\n",
    "indexer.setProperties(**props)\n",
    "indexref = indexer.index(dataset.get_corpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the statistics of the generated index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T11:31:51.841120Z",
     "iopub.status.busy": "2020-08-31T11:31:51.840341Z",
     "iopub.status.idle": "2020-08-31T11:31:52.498032Z",
     "shell.execute_reply": "2020-08-31T11:31:52.497236Z",
     "shell.execute_reply.started": "2020-08-31T11:31:51.841035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3213835\n",
      "Number of terms: 17470544\n",
      "Number of fields: 0\n",
      "Number of tokens: 3667907097\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pt.logging('WARN')\n",
    "index = pt.IndexFactory.of(indexref)\n",
    "print(index.getCollectionStatistics().toString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All being well, you should have indexed 3213835 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "This notebook contains a demonstration of how to execute a baseline retrieval run, using a Divergence from Randomness weighting model called [DPH](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/DPH.html). You could also use [BM25](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/BM25.html) or many of the [other weighting models that Terrier provides](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html).\n",
    "\n",
    "We use an object called BatchRetrieve. The constructor parameters are as follows:\n",
    " - `wmodel` - name of the Terrier weighting model class\n",
    " - `properties` - Terrier configurations - here we re-specify the termpipeline to match the indexing configuration\n",
    " - `verbose` - we set this to True, so we can view progress (using [TQDM](https://github.com/tqdm/tqdm)) when retrieving for these large topic sets.\n",
    " \n",
    "Finally, we only want 100 results per query, so we apply the rank cutoff operator `%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T11:31:57.606705Z",
     "iopub.status.busy": "2020-08-31T11:31:57.606043Z",
     "iopub.status.idle": "2020-08-31T11:31:57.631259Z",
     "shell.execute_reply": "2020-08-31T11:31:57.630256Z",
     "shell.execute_reply.started": "2020-08-31T11:31:57.606470Z"
    }
   },
   "outputs": [],
   "source": [
    "DPH_br = pt.BatchRetrieve(index, wmodel=\"DPH\", properties={\"termpipelines\": \"\"}, verbose=True) % 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now evaluate performance on the MSMARCO Dev set. Experiment is a declarative notation for running one or more experiment pipelines on a standard set of topics, then evaluating them for the same qrels. We report the MRR measure.\n",
    "\n",
    "The dev set is quite large (> 5000 queries). This took 1 hour to run for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T11:32:40.096317Z",
     "iopub.status.busy": "2020-08-31T11:32:40.095814Z",
     "iopub.status.idle": "2020-08-31T12:34:45.657244Z",
     "shell.execute_reply": "2020-08-31T12:34:45.656121Z",
     "shell.execute_reply.started": "2020-08-31T11:32:40.096238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:32:40.100 [main] WARN  o.t.a.batchquerying.TRECQuery - trec.encoding is not set; resorting to platform default (ISO-8859-1). Retrieval may be platform dependent. Recommend trec.encoding=UTF-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5193/5193 [1:01:49<00:00,  1.40q/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>recip_rank</th>\n",
       "      <th>map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BR(DPH)</td>\n",
       "      <td>0.25827</td>\n",
       "      <td>0.25827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  recip_rank      map\n",
       "0  BR(DPH)     0.25827  0.25827"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment([DPH_br], dataset.get_topics(\"dev\"), dataset.get_qrels(\"dev\"), eval_metrics=[\"recip_rank\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Leaderboard results\n",
    "\n",
    "Finally, lets prepare a results file for sending to the leaderboard. Again, with 5793 topics, this took about 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-31T12:34:45.659500Z",
     "iopub.status.busy": "2020-08-31T12:34:45.659166Z",
     "iopub.status.idle": "2020-08-31T13:43:32.409255Z",
     "shell.execute_reply": "2020-08-31T13:43:32.408151Z",
     "shell.execute_reply.started": "2020-08-31T12:34:45.659442Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5793/5793 [1:07:51<00:00,  1.42q/s]\n"
     ]
    }
   ],
   "source": [
    "pt.io.write_results(DPH_br(dataset.get_topics(\"leaderboard-2020\")), \"uogTrBaseDPH.res.gz\", run_name=\"uogTrBaseDPH\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyterrier",
   "language": "python",
   "name": "pyterrier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
